{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_func import activation_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP(Rmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP import MLP\n",
    "from Initial_weights import Init_Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2022 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Init_Weights object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m de_output_act \u001b[39m=\u001b[39m activation_func\u001b[39m.\u001b[39md_identity\n\u001b[1;32m     20\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m mlp \u001b[39m=\u001b[39m MLP(\n\u001b[1;32m     22\u001b[0m     input_size,\n\u001b[1;32m     23\u001b[0m     hidden_size,\n\u001b[1;32m     24\u001b[0m     output_size,\n\u001b[1;32m     25\u001b[0m     activation_func_hidden\u001b[39m=\u001b[39;49mhidden_act,\n\u001b[1;32m     26\u001b[0m     activation_func_output\u001b[39m=\u001b[39;49moutput_act,\n\u001b[1;32m     27\u001b[0m     derivative_act_hidden\u001b[39m=\u001b[39;49mde_hidden_act,\n\u001b[1;32m     28\u001b[0m     derivative_act_output\u001b[39m=\u001b[39;49mde_output_act,\n\u001b[1;32m     29\u001b[0m     batch_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     30\u001b[0m     bias\u001b[39m=\u001b[39;49mbias,\n\u001b[1;32m     31\u001b[0m     print_steps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     32\u001b[0m     decimal_point\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     init_weights\u001b[39m=\u001b[39;49mInit_Weights(input_size, hidden_size, output_size, bias)\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m v,w \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39mtrain(X, y, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/University/Neural Networks and Deep Learning/homework/NN-models/MLP.py:39\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[0;34m(self, input_size, hidden_size, output_size, activation_func_hidden, activation_func_output, derivative_act_hidden, derivative_act_output, bias, batch_mode, print_steps, decimal_point, init_weights)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2 \u001b[39m=\u001b[39m init_weights\u001b[39m.\u001b[39mfill_equal(v_h\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, v_o\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2 \u001b[39m=\u001b[39m init_weights\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelta_W1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelta_W2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable Init_Weights object"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.25, 0.5, 0.75], [0.75, 0.5, 0.25]])\n",
    "y = np.array([[0.25], [0.75]])\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 2\n",
    "output_size = y.shape[1]\n",
    "\n",
    "V = np.full((input_size, hidden_size), 0.5)\n",
    "W = np.full((hidden_size, output_size), 0.5)\n",
    "\n",
    "# V0 = np.full((1, hidden_size), 0.5)\n",
    "# W0 = np.full((1, output_size), 0.5)\n",
    "\n",
    "# V = np.vstack((V0, V))\n",
    "# W = np.vstack((W0, W))\n",
    "\n",
    "hidden_act = activation_func.bipolar_sigmoid\n",
    "de_hidden_act = activation_func.d_bipolar_sigmoid\n",
    "output_act = activation_func.identity\n",
    "de_output_act = activation_func.d_identity\n",
    "bias = False\n",
    "mlp = MLP(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    activation_func_hidden=hidden_act,\n",
    "    activation_func_output=output_act,\n",
    "    derivative_act_hidden=de_hidden_act,\n",
    "    derivative_act_output=de_output_act,\n",
    "    batch_mode=True,\n",
    "    bias=bias,\n",
    "    print_steps=True,\n",
    "    decimal_point=3,\n",
    "    init_weights=None,\n",
    ")\n",
    "v,w = mlp.train(X, y, epochs=1, learning_rate=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_ULTRA import MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2022 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "╒════════════════════╤═══════════════╤═════════════════╤════════╤═══════╕\n",
      "│ x                  │ z_in          │ z               │   y_in │     y │\n",
      "╞════════════════════╪═══════════════╪═════════════════╪════════╪═══════╡\n",
      "│ [[0.25 0.5  0.75]] │ [[0.75 0.75]] │ [[0.358 0.358]] │  0.358 │ 0.358 │\n",
      "├────────────────────┼───────────────┼─────────────────┼────────┼───────┤\n",
      "│ [[0.75 0.5  0.25]] │ [[0.75 0.75]] │ [[0.358 0.358]] │  0.358 │ 0.358 │\n",
      "╘════════════════════╧═══════════════╧═════════════════╧════════╧═══════╛\n",
      "╒═══════════════════╤════════╤═══════════════════╤═══════════╕\n",
      "│ δ_H               │    δ_O │ h(q)              │ o(q)      │\n",
      "╞═══════════════════╪════════╪═══════════════════╪═══════════╡\n",
      "│ [[0.024 0.024]]   │  0.108 │ [[0.006 0.006]    │ [[0.039]  │\n",
      "│                   │        │  [0.012 0.012]    │  [0.039]] │\n",
      "│                   │        │  [0.018 0.018]]   │           │\n",
      "├───────────────────┼────────┼───────────────────┼───────────┤\n",
      "│ [[-0.085 -0.085]] │ -0.392 │ [[-0.064 -0.064]  │ [[-0.14]  │\n",
      "│                   │        │  [-0.043 -0.043]  │  [-0.14]] │\n",
      "│                   │        │  [-0.021 -0.021]] │           │\n",
      "╘═══════════════════╧════════╧═══════════════════╧═══════════╛\n",
      "o = [[-0.102]\n",
      " [-0.102]]\n",
      "h = [[-0.058 -0.058]\n",
      " [-0.031 -0.031]\n",
      " [-0.004 -0.004]]\n",
      "Δv = [[0.029 0.029]\n",
      " [0.015 0.015]\n",
      " [0.002 0.002]]\n",
      "Δw = [[0.051]\n",
      " [0.051]]\n",
      "v = [[0.529 0.529]\n",
      " [0.515 0.515]\n",
      " [0.502 0.502]]\n",
      "w = [[0.551]\n",
      " [0.551]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hesoyam/Documents/University/Neural Networks and Deep Learning/homework/NN-models/venv/lib/python3.9/site-packages/tabulate/__init__.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  (len(row) >= 1 and row[0] == SEPARATING_LINE)\n",
      "/Users/hesoyam/Documents/University/Neural Networks and Deep Learning/homework/NN-models/venv/lib/python3.9/site-packages/tabulate/__init__.py:108: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  or (len(row) >= 2 and row[1] == SEPARATING_LINE)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.25, 0.5, 0.75], [0.75, 0.5, 0.25]])\n",
    "y = np.array([[0.25], [0.75]])\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 2\n",
    "output_size = y.shape[1]\n",
    "\n",
    "V = np.full((input_size, hidden_size), 0.5)\n",
    "W = np.full((hidden_size, output_size), 0.5)\n",
    "\n",
    "# V0 = np.full((1, hidden_size), 0.5)\n",
    "# W0 = np.full((1, output_size), 0.5)\n",
    "\n",
    "# V = np.vstack((V0, V))\n",
    "# W = np.vstack((W0, W))\n",
    "\n",
    "hidden_act = activation_func.bipolar_sigmoid\n",
    "de_hidden_act = activation_func.d_bipolar_sigmoid\n",
    "output_act = activation_func.identity\n",
    "de_output_act = activation_func.d_identity\n",
    "mlp = MLP(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    activation_func_hidden=hidden_act,\n",
    "    activation_func_output=output_act,\n",
    "    derivative_act_hidden=de_hidden_act,\n",
    "    derivative_act_output=de_output_act,\n",
    "    batch_mode=True,\n",
    "    bias=False,\n",
    "    print_steps=True,\n",
    "    decimal_point=3,\n",
    "    init_weights=(V, W)\n",
    ")\n",
    "v,w = mlp.train(X, y, epochs=1, learning_rate=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "╒════════════════════╤═══════════╤═════════════════╤════════╤═══════╕\n",
      "│ x                  │ z_in      │ z               │   y_in │     y │\n",
      "╞════════════════════╪═══════════╪═════════════════╪════════╪═══════╡\n",
      "│ [[0.25 0.5  0.25]] │ [[1. 1.]] │ [[0.462 0.462]] │  0.962 │ 0.962 │\n",
      "├────────────────────┼───────────┼─────────────────┼────────┼───────┤\n",
      "│ [[0.5  0.25 0.5 ]] │ [[1. 1.]] │ [[0.462 0.462]] │  0.303 │ 0.303 │\n",
      "╘════════════════════╧═══════════╧═════════════════╧════════╧═══════╛\n",
      "╒═════════════════╤════════╤═══════════════════╤════════════╤═════════════════╤═══════════╕\n",
      "│ δ_H             │    δ_O │ Δv                │ Δw         │ v               │ w         │\n",
      "╞═════════════════╪════════╪═══════════════════╪════════════╪═════════════════╪═══════════╡\n",
      "│ [[0.091 0.091]] │  0.462 │ [[-0.091 -0.091]  │ [[-0.462]  │ [[0.409 0.409]  │ [[0.038]  │\n",
      "│                 │        │  [-0.023 -0.023]  │  [-0.214]  │  [0.477 0.477]  │  [0.286]  │\n",
      "│                 │        │  [-0.045 -0.045]  │  [-0.214]] │  [0.455 0.455]  │  [0.286]] │\n",
      "│                 │        │  [-0.023 -0.023]] │            │  [0.477 0.477]] │           │\n",
      "├─────────────────┼────────┼───────────────────┼────────────┼─────────────────┼───────────┤\n",
      "│ [[-0.05 -0.05]] │ -0.447 │ [[0.05  0.05 ]    │ [[0.447]   │ [[0.46  0.46 ]  │ [[0.485]  │\n",
      "│                 │        │  [0.025 0.025]    │  [0.207]   │  [0.502 0.502]  │  [0.493]  │\n",
      "│                 │        │  [0.013 0.013]    │  [0.207]]  │  [0.467 0.467]  │  [0.493]] │\n",
      "│                 │        │  [0.025 0.025]]   │            │  [0.502 0.502]] │           │\n",
      "╘═════════════════╧════════╧═══════════════════╧════════════╧═════════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.25, 0.5, 0.25], [0.5, 0.25, 0.5]])\n",
    "y = np.array([[0.5], [0.75]])\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 2\n",
    "output_size = y.shape[1]\n",
    "\n",
    "V = np.full((input_size, hidden_size), 0.5)\n",
    "W = np.full((hidden_size, output_size), 0.5)\n",
    "\n",
    "V0 = np.full((1, hidden_size), 0.5)\n",
    "W0 = np.full((1, output_size), 0.5)\n",
    "\n",
    "V = np.vstack((V0, V))\n",
    "W = np.vstack((W0, W))\n",
    "\n",
    "hidden_act = activation_func.bipolar_sigmoid\n",
    "de_hidden_act = activation_func.d_bipolar_sigmoid\n",
    "output_act = activation_func.identity\n",
    "de_output_act = activation_func.d_identity\n",
    "mlp = MLP(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    activation_func_hidden=hidden_act,\n",
    "    activation_func_output=output_act,\n",
    "    derivative_act_hidden=de_hidden_act,\n",
    "    derivative_act_output=de_output_act,\n",
    "    batch_mode=False,\n",
    "    bias=True,\n",
    "    print_steps=True,\n",
    "    decimal_point=3,\n",
    "    init_weights=(V, W)\n",
    ")\n",
    "v, w = mlp.train(X, y, epochs=1, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "╒══════════════╤═══════════════╤═══════════════╤═══════════════╤═══════════════╕\n",
      "│ x            │ z_in          │ z             │ y_in          │ y             │\n",
      "╞══════════════╪═══════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
      "│ [[1 1 1]]    │ [[1.48 1.33]] │ [[0.63 0.58]] │ [[0.67 1.52]] │ [[0.67 1.52]] │\n",
      "├──────────────┼───────────────┼───────────────┼───────────────┼───────────────┤\n",
      "│ [[ 1 -1 -1]] │ [[0.36 0.23]] │ [[0.18 0.11]] │ [[0.32 1.02]] │ [[0.32 1.02]] │\n",
      "╘══════════════╧═══════════════╧═══════════════╧═══════════════╧═══════════════╛\n",
      "╒═════════════════╤═════════════════╤═════════════════╤═════════════════╤═══════════════╤═══════════════╕\n",
      "│ δ_H             │ δ_O             │ Δv              │ Δw              │ v             │ w             │\n",
      "╞═════════════════╪═════════════════╪═════════════════╪═════════════════╪═══════════════╪═══════════════╡\n",
      "│ [[-0.1  -0.03]] │ [[-0.3  -0.11]] │ [[0.02 0.01]    │ [[0.06 0.02]    │ [[0.24 0.58]  │ [[0.15 0.83]  │\n",
      "│                 │                 │  [0.02 0.01]    │  [0.04 0.01]    │  [0.72 0.22]  │  [0.81 0.87]  │\n",
      "│                 │                 │  [0.02 0.01]    │  [0.04 0.01]]   │  [0.2  0.12]  │  [0.21 0.3 ]] │\n",
      "│                 │                 │  [0.02 0.01]]   │                 │  [0.4  0.45]] │               │\n",
      "├─────────────────┼─────────────────┼─────────────────┼─────────────────┼───────────────┼───────────────┤\n",
      "│ [[0.46 0.14]]   │ [[0.56 0.57]]   │ [[-0.09 -0.03]  │ [[-0.11 -0.11]  │ [[0.15 0.55]  │ [[0.04 0.72]  │\n",
      "│                 │                 │  [-0.09 -0.03]  │  [-0.02 -0.02]  │  [0.63 0.19]  │  [0.79 0.85]  │\n",
      "│                 │                 │  [ 0.09  0.03]  │  [-0.01 -0.01]] │  [0.29 0.14]  │  [0.19 0.29]] │\n",
      "│                 │                 │  [ 0.09  0.03]] │                 │  [0.49 0.47]] │               │\n",
      "╘═════════════════╧═════════════════╧═════════════════╧═════════════════╧═══════════════╧═══════════════╛\n"
     ]
    }
   ],
   "source": [
    "input_size = 3\n",
    "hidden_size = 2\n",
    "output_size = 2\n",
    "X = np.array([[1, 1, 1], [1, -1, -1]])\n",
    "y = np.array([[1, 1], [-1, -1]])\n",
    "\n",
    "V = np.array([\n",
    "    [0.22, 0.57],\n",
    "    [0.7, 0.21],\n",
    "    [0.18, 0.11],\n",
    "    [0.38, 0.44]\n",
    "])\n",
    "\n",
    "W = np.array([\n",
    "    [0.09, 0.81],\n",
    "    [0.77, 0.86],\n",
    "    [0.17, 0.29]\n",
    "])\n",
    "\n",
    "hidden_act = activation_func.bipolar_sigmoid\n",
    "de_hidden_act = activation_func.d_bipolar_sigmoid\n",
    "output_act = activation_func.bipolar_sigmoid\n",
    "de_output_act = activation_func.d_bipolar_sigmoid\n",
    "\n",
    "mlp = MLP(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    activation_func_hidden=hidden_act,\n",
    "    activation_func_output=output_act,\n",
    "    derivative_act_hidden=de_hidden_act,\n",
    "    derivative_act_output=de_output_act,\n",
    "    batch_mode=False,\n",
    "    bias=True,\n",
    "    print_steps=True,\n",
    "    init_weights=(V, W)\n",
    ")\n",
    "v, w = mlp.train(X, y, epochs=1, learning_rate=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_func import activation_func\n",
    "from pooling import pooling\n",
    "from convNet import convNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_in(1) = -3.45\n",
      "Z_in(2) = -2.5\n",
      "Z_in(3) = -1.5\n",
      "Z_in(4) = -0.25\n",
      "Z_in(5) = 0.65\n",
      "Z_in(6) = 1.9\n",
      "Z_in = [-3.45 -2.5  -1.5  -0.25  0.65  1.9 ]\n",
      "After applying hidden layer activation function:\n",
      "Z = [-0.968 -0.918 -0.777 -0.221  0.65   1.9  ]\n",
      "Output after pooling:\n",
      "y = [-0.943 -0.499  1.275]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-2.0, -1.8, -1.4, -0.6, -0.2, 0.4, 0.8, 1.6])\n",
    "\n",
    "conv_filter = np.array([0.75, 0.5, 0.75])\n",
    "\n",
    "conv1d = convNet(\n",
    "    conv_filter=conv_filter,\n",
    "    stride_conv=1,\n",
    "    pooling_type=pooling.average_pooling,\n",
    "    pooling_size=2,\n",
    "    stride_pooling=2,\n",
    "    decimal_point=3,\n",
    "    act_f=activation_func.Elu,\n",
    ")\n",
    "\n",
    "out_H, out_O = conv1d.forward(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_in(1) = -3.45\n",
      "Z_in(2) = -2.5\n",
      "Z_in(3) = -1.5\n",
      "Z_in(4) = -0.25\n",
      "Z_in(5) = 0.65\n",
      "Z_in(6) = 1.9\n",
      "Z_in = [-3.45 -2.5  -1.5  -0.25  0.65  1.9 ]\n",
      "After applying hidden layer activation function:\n",
      "Z = [-0.968 -0.918 -0.777 -0.221  0.65   1.9  ]\n",
      "Output after pooling:\n",
      "y = [-0.933 -0.388  1.525]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-2.0, -1.8, -1.4, -0.6, -0.2, 0.4, 0.8, 1.6])\n",
    "\n",
    "conv_filter = np.array([0.75, 0.5, 0.75])\n",
    "weights = [0.3, 0.7]\n",
    "\n",
    "conv1d = convNet(\n",
    "    conv_filter=conv_filter,\n",
    "    stride_conv=1,\n",
    "    pooling_type=pooling.weighted_pooling,\n",
    "    pooling_params=weights,\n",
    "    pooling_size=2,\n",
    "    stride_pooling=2,\n",
    "    decimal_point=3,\n",
    "    act_f=activation_func.Elu,\n",
    ")\n",
    "\n",
    "out_H, out_O = conv1d.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffc2c986650f75bb84df5ef0f5794d173c138677d61245fd2c4ff2debf2f2371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
